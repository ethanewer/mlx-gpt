{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from typing import Generator, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from mlx import core as mx, nn, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are chars, so the vocab size is the number of unique chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char2int = {c: i for i, c in enumerate(chars)}\n",
    "int2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    return [char2int[c] for c in s if c in char2int]\n",
    "\n",
    "\n",
    "def decode(y: list[int] | np.ndarray | mx.array) -> str:\n",
    "    if isinstance(y, mx.array):\n",
    "        y = np.array(y)\n",
    "    return \"\".join([int2char[int(i)] for i in y if int(i) in int2char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text is encoded as an `mx.array`, then split into training and validation\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = mx.array(encode(text), dtype=mx.int64)\n",
    "\n",
    "val_size = len(full_data) // 10\n",
    "\n",
    "train_data = full_data[val_size:]\n",
    "val_data = full_data[:val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes data into blocks\n",
    "\n",
    "$x_i = [d_i, d_{i + 1}, ..., d_{i + b}]$\n",
    "\n",
    "$y_i = [d_{i + 1}, d_{i + 2}, ..., d_{i + b + 1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_data(data: mx.array, block_size: int) -> tuple[mx.array, mx.array]:\n",
    "    n_blocks = len(data) - block_size - 1\n",
    "    x = mx.stack([data[i:i + block_size] for i in range(n_blocks)])\n",
    "    y = mx.stack([data[i:i + block_size] for i in range(1, n_blocks + 1)])\n",
    "    mx.eval(x, y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates random batches given $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterate(\n",
    "    x: mx.array, \n",
    "    y: mx.array, \n",
    "    batch_size: int,\n",
    ") -> Generator[tuple[mx.array, mx.array], None, None]:\n",
    "    permutation = mx.array(np.random.permutation(y.shape[0]))\n",
    "    for s in range(0, y.shape[0], batch_size):\n",
    "        idxs = permutation[s:s + batch_size]\n",
    "        bx = x[idxs]\n",
    "        by = y[idxs]\n",
    "        mx.eval(bx, by)\n",
    "        yield bx, by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "$y = \\frac{x - E[x]}{\\sqrt{E[(x - E[x])^2] + \\epsilon}} \\odot \\gamma + \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: Sequence[int], bias=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = mx.ones(normalized_shape)\n",
    "        self.bias = mx.ones(normalized_shape) if bias else None\n",
    "        self.eps = eps\n",
    "    \n",
    "\n",
    "    def __call__(self, x: mx.array): \n",
    "        return mx.fast.layer_norm(x, self.weight, self.bias, self.eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "$[q_{i, j}, k_{i, j}, v_{i, j}] = x_{i, j}[W_q, W_k, W_v] + [b_q, b_k, b_v]$ \n",
    "where all $q, k, v, x, b$ are row vectors\n",
    "\n",
    "$[q_{i, j}, k_{i, j}, v_{i, j}]$ are computed for $x_i$ in the 3D tensor \n",
    "$x = \\begin{bmatrix}\n",
    "x_{1, 1} & \\dots & x_{1, T} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{B, 1} & \\dots & x_{B, T}\n",
    "\\end{bmatrix}$ resulting in tensors $q, k, v$\n",
    "\n",
    "$x$ has shape $(B, T, C)$ where $B$ is the batch size, $T$ is the sequence length,\n",
    "and $C$ is the number of embedding dimensions\n",
    "\n",
    "$q, k, v$ have shape $(B, T, ND)$ where $B$ is the batch size, \n",
    "$T$ is the sequence length, $N$ is the number of attention heads, and $D$ is the\n",
    "number of query/key dimensions\n",
    "\n",
    "$q, k, v$ are reshaped to $(B, N, T, D)$\n",
    "\n",
    "$a_{i, j} = q_{i, j} k_{i, j}^T$\n",
    "\n",
    "$a_{i, j} = -\\infty$ for all $i < j$\n",
    "\n",
    "$a_{i, j} = \\text{softmax}(a_{i, j})$ where softmax is computed rowwise\n",
    "\n",
    "$y_{i, j} = a_{i, j} v_{i, j}$\n",
    "\n",
    "$y$ has shape $(B, N, T, D)$\n",
    "\n",
    "$y$ is reshaped to $(B, T, ND)$, so $y_{i, j}$ is a row vector\n",
    "\n",
    "$y_{i, j} = y_{i, j}W_p + b_p$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_head: int, dropout: float, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        assert n_embed % n_head == 0 \n",
    "        self.D = n_embed // n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_scale = 1.0 / np.sqrt(self.D)\n",
    "    \n",
    "\n",
    "    def __call__(self, x: mx.array):\n",
    "        B, T, n_embed = x.shape\n",
    "        assert n_embed == self.n_embed\n",
    "\n",
    "        tmp = self.c_attn(x)\n",
    "        tmp = tmp.split(self.n_embed, axis=2)\n",
    "\n",
    "        q, k, v = mx.split(self.c_attn(x), 3, axis=2)\n",
    "\n",
    "        # reshape to (B, N, T, D)\n",
    "        q = q.reshape((B, T, self.n_head, self.D)).transpose((0, 2, 1, 3)) \n",
    "        k = k.reshape((B, T, self.n_head, self.D)).transpose((0, 2, 1, 3))\n",
    "        v = v.reshape((B, T, self.n_head, self.D)).transpose((0, 2, 1, 3))\n",
    "        \n",
    "        y = mx.fast.scaled_dot_product_attention(q, k, v, scale=self.attn_scale)\n",
    "        y = y.transpose((0, 2, 1, 3)).reshape((B, T, self.n_embed)) # concat head outputs \n",
    "        y = self.c_proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "$x_{i, j} = x_{i, j} W_{c} + B_{c}$\n",
    "\n",
    "$y_{i, j} = x_{i, j} W_{p} + B_{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embed: int, dropout: float, bias=True):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_embed, 4 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(4 * n_embed, n_embed, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def __call__(self, x: mx.array):\n",
    "        x = nn.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block\n",
    "\n",
    "Composition of layer normalization, self attention, and mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_head: int, dropout: float, bias=True):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_embed, bias=bias)\n",
    "        self.attn = SelfAttention(n_embed, n_head, dropout, bias)\n",
    "        self.ln_2 = LayerNorm(n_embed, bias=bias)\n",
    "        self.mlp = MLP(n_embed, dropout, bias)\n",
    "\n",
    "\n",
    "    def __call__(self, x: mx.array):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Transformer\n",
    "- Input is an array of token indexes\n",
    "- Computes token embeddings from the input\n",
    "- Computes position embeddings from the sequence $[0, 1, ..., T - 1]$\n",
    "- $x$ is the sum of the token and position embeddings\n",
    "- $x$ is forwarded through all the blocks\n",
    "- $x$ is layer normalized one more time\n",
    "- $x$ is forwarded through a linear layer to transform it from the embedding dimension \n",
    "    to the vocab size\n",
    "- If generating, $p = \\text{softmax}(x)$, and the next index is drawn from the \n",
    "    distribution $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, n_head: int, block_size: int, \n",
    "        vocab_size: int, n_layer: int, dropout: float, bias=True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.wte = nn.Embedding(vocab_size, n_embed)\n",
    "        self.wpe = nn.Embedding(block_size, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.h = [Block(n_embed, n_head, dropout, bias) for _ in range(n_layer)]\n",
    "        self.ln_f = LayerNorm(n_embed, bias=bias)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.wte.weight = self.lm_head.weight\n",
    "\n",
    "        def init_weights(_, m: nn.Module):\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Embedding):\n",
    "                m.weight = nn.init.normal(0.0, 0.02)(m.weight)\n",
    "                if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                    m.bias = mx.zeros_like(m.bias)\n",
    "        \n",
    "\n",
    "        self.apply_to_modules(init_weights)\n",
    "                \n",
    "    \n",
    "\n",
    "    def __call__(self, x_idx: mx.array):\n",
    "        _, T = x_idx.shape\n",
    "\n",
    "        assert T <= self.block_size, \\\n",
    "            f\"cannot forward sequence of length {T}, block size is only {self.block_size}\"\n",
    "        \n",
    "        pos = mx.arange(0, T, dtype=mx.int64)\n",
    "\n",
    "        tok_emb = self.wte(x_idx) # shape (B, T, C)\n",
    "        pos_emb = self.wpe(pos) # shape (T, C)\n",
    "\n",
    "        # (B, T, C) + (T, C) = (B, T, C)\n",
    "        # elementwise addition for each batch\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        for blk in self.h:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def generate(self, x_idx: mx.array, max_new_tokens: int, temperature=1.0):\n",
    "        # Take a conditioning sequence of indices x_idx (int64 tensor of shape (B, T)) and \n",
    "        # complete the sequence max_new_tokens times, feeding the predictions back into \n",
    "        # the model each time. Most likely you\"ll want to make sure to be in model.eval() \n",
    "        # mode of operation for this.\n",
    "        for _ in range(max_new_tokens):\n",
    "            if x_idx.shape[1] <= self.block_size:\n",
    "                x_idx_cropped = x_idx \n",
    "            else:\n",
    "                x_idx_cropped = x_idx[:, -self.block_size:]\n",
    "\n",
    "            logits = self(x_idx_cropped)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            next_idx = mx.random.categorical(logits)[None]\n",
    "            x_idx = mx.concatenate((x_idx, next_idx), axis=1)\n",
    "        return x_idx  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_INTERVAL = 1000\n",
    "LOG_INTERVAL = 100\n",
    "\n",
    "BLOCK_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "MAX_ITERS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to blocks\n",
    "\n",
    "$x_i = [d_i, d_{i + 1}, ..., d_{i + b}]$\n",
    "\n",
    "$y_i = [d_{i + 1}, d_{i + 2}, ..., d_{i + b + 1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = block_data(train_data, BLOCK_SIZE)\n",
    "x_val, y_val = block_data(val_data, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model, optimizer, and training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeTransformer(\n",
    "    n_embed=768, \n",
    "    n_head=12, \n",
    "    block_size=BLOCK_SIZE, \n",
    "    vocab_size=50304,\n",
    "    n_layer=12, \n",
    "    dropout=0.0, \n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "model.load_weights(\"checkpoints/model.npz\")\n",
    "\n",
    "optimizer = optimizers.AdamW(1e-3, (0.9, 0.95), 1e-7, 0.1)\n",
    "\n",
    "state = [model.state, optimizer.state]\n",
    "\n",
    "mx.eval(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy loss:\n",
    "\n",
    "$l(x, y, \\theta) = -\\sum_i y_i \\log(f(x_i, \\theta))$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Train Step with Adam Optimizer\n",
    "\n",
    "$g_t = \\nabla_{\\theta_{t - 1}} l(x, y, )$\n",
    "\n",
    "$\\alpha = \\eta \\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}$\n",
    "\n",
    "$m_t = \\beta_1 m_{t - 1} + (1 - \\beta_1)g_t$\n",
    "\n",
    "$m_t = \\beta_2 v_{t - 1} + (1 - \\beta_2)g_t^2$\n",
    "\n",
    "$\\theta_t = \\theta_{t - 1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    return nn.losses.cross_entropy(model(x), y, reduction=\"mean\")\n",
    "\n",
    "\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def train_step(x, y):\n",
    "    loss, grads = nn.value_and_grad(model, loss_fn)(model, x, y)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@partial(mx.compile, inputs=state)\n",
    "def eval_step(x, y):\n",
    "    return loss_fn(model, x, y)\n",
    "\n",
    "\n",
    "def evaluate_loss(x, y, max_iters=4):\n",
    "    loss_sum = 0\n",
    "    cnt = 0\n",
    "    for i, (bx, by) in enumerate(batch_iterate(x, y, BATCH_SIZE)):\n",
    "        if i >= max_iters:\n",
    "            break\n",
    "        loss = eval_step(bx, by)\n",
    "        loss_sum += loss * len(x)\n",
    "        cnt += len(x)\n",
    "    loss = loss_sum / cnt\n",
    "    mx.eval(loss)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100] loss: 3.333, time: 17.012s\n",
      "[ 200] loss: 2.884, time: 16.914s\n",
      "[ 300] loss: 2.494, time: 16.943s\n",
      "[ 400] loss: 2.553, time: 16.987s\n",
      "[ 500] loss: 2.355, time: 16.979s\n",
      "[ 600] loss: 2.277, time: 16.979s\n",
      "[ 700] loss: 2.400, time: 16.988s\n",
      "[ 800] loss: 2.331, time: 16.986s\n",
      "[ 900] loss: 2.251, time: 16.968s\n",
      "[1000] loss: 2.270, time: 16.988s\n",
      "train loss: 2.2220, val loss: 2.2195\n",
      "[1100] loss: 2.220, time: 17.535s\n",
      "[1200] loss: 2.108, time: 16.984s\n",
      "[1300] loss: 2.108, time: 17.004s\n",
      "[1400] loss: 2.135, time: 16.995s\n",
      "[1500] loss: 2.109, time: 16.985s\n",
      "[1600] loss: 2.221, time: 17.002s\n",
      "[1700] loss: 2.157, time: 17.018s\n",
      "[1800] loss: 2.049, time: 17.036s\n",
      "[1900] loss: 2.144, time: 17.021s\n",
      "[2000] loss: 2.070, time: 16.971s\n",
      "train loss: 2.1046, val loss: 2.1848\n",
      "[2100] loss: 2.181, time: 17.510s\n",
      "[2200] loss: 2.029, time: 16.988s\n",
      "[2300] loss: 1.997, time: 16.986s\n",
      "[2400] loss: 2.047, time: 16.980s\n",
      "[2500] loss: 2.005, time: 16.991s\n",
      "[2600] loss: 2.049, time: 17.007s\n",
      "[2700] loss: 2.033, time: 16.942s\n",
      "[2800] loss: 2.039, time: 17.002s\n",
      "[2900] loss: 2.109, time: 16.977s\n",
      "[3000] loss: 1.901, time: 16.984s\n",
      "train loss: 1.9988, val loss: 2.0315\n",
      "[3100] loss: 1.994, time: 17.489s\n",
      "[3200] loss: 2.030, time: 17.012s\n",
      "[3300] loss: 1.861, time: 16.992s\n",
      "[3400] loss: 1.969, time: 17.009s\n",
      "[3500] loss: 1.967, time: 16.982s\n",
      "[3600] loss: 1.959, time: 16.968s\n",
      "[3700] loss: 1.951, time: 16.983s\n",
      "[3800] loss: 1.837, time: 16.964s\n",
      "[3900] loss: 1.871, time: 16.992s\n",
      "[4000] loss: 1.896, time: 16.991s\n",
      "train loss: 1.9551, val loss: 1.9640\n",
      "[4100] loss: 1.938, time: 17.493s\n",
      "[4200] loss: 2.036, time: 16.982s\n",
      "[4300] loss: 1.886, time: 16.975s\n",
      "[4400] loss: 1.929, time: 16.976s\n",
      "[4500] loss: 1.938, time: 16.983s\n",
      "[4600] loss: 1.890, time: 16.996s\n",
      "[4700] loss: 1.843, time: 16.981s\n",
      "[4800] loss: 1.971, time: 16.964s\n",
      "[4900] loss: 1.931, time: 16.974s\n",
      "[5000] loss: 1.953, time: 16.982s\n",
      "train loss: 1.8567, val loss: 1.9518\n",
      "[5100] loss: 1.896, time: 17.481s\n",
      "[5200] loss: 1.984, time: 16.981s\n",
      "[5300] loss: 1.847, time: 17.004s\n",
      "[5400] loss: 1.869, time: 16.966s\n",
      "[5500] loss: 1.812, time: 16.982s\n",
      "[5600] loss: 1.946, time: 16.973s\n",
      "[5700] loss: 1.912, time: 16.967s\n",
      "[5800] loss: 1.800, time: 16.973s\n",
      "[5900] loss: 1.947, time: 16.982s\n",
      "[6000] loss: 1.916, time: 16.971s\n",
      "train loss: 1.8741, val loss: 1.9388\n",
      "[6100] loss: 1.855, time: 17.491s\n",
      "[6200] loss: 1.870, time: 16.968s\n",
      "[6300] loss: 1.920, time: 17.000s\n",
      "[6400] loss: 1.824, time: 16.988s\n",
      "[6500] loss: 1.890, time: 17.007s\n",
      "[6600] loss: 1.834, time: 16.962s\n",
      "[6700] loss: 1.779, time: 16.984s\n",
      "[6800] loss: 1.794, time: 16.985s\n",
      "[6900] loss: 1.896, time: 16.997s\n",
      "[7000] loss: 1.783, time: 17.120s\n",
      "train loss: 1.8700, val loss: 1.9644\n",
      "[7100] loss: 1.776, time: 17.464s\n",
      "[7200] loss: 1.858, time: 17.144s\n",
      "[7300] loss: 1.894, time: 17.135s\n",
      "[7400] loss: 1.876, time: 17.156s\n",
      "[7500] loss: 1.816, time: 17.121s\n",
      "[7600] loss: 1.834, time: 17.144s\n",
      "[7700] loss: 1.869, time: 17.170s\n",
      "[7800] loss: 1.925, time: 17.150s\n",
      "[7900] loss: 1.877, time: 17.163s\n",
      "[8000] loss: 1.813, time: 17.130s\n",
      "train loss: 1.8105, val loss: 1.8416\n",
      "[8100] loss: 1.731, time: 17.692s\n",
      "[8200] loss: 1.809, time: 17.144s\n",
      "[8300] loss: 1.869, time: 17.126s\n",
      "[8400] loss: 1.709, time: 17.146s\n",
      "[8500] loss: 1.761, time: 17.122s\n",
      "[8600] loss: 1.846, time: 17.141s\n",
      "[8700] loss: 1.802, time: 17.143s\n",
      "[8800] loss: 1.803, time: 17.146s\n",
      "[8900] loss: 1.832, time: 17.143s\n",
      "[9000] loss: 1.747, time: 17.167s\n",
      "train loss: 1.8416, val loss: 1.7919\n",
      "[9100] loss: 1.673, time: 17.678s\n",
      "[9200] loss: 1.737, time: 17.166s\n",
      "[9300] loss: 1.700, time: 17.195s\n",
      "[9400] loss: 1.820, time: 17.159s\n",
      "[9500] loss: 1.815, time: 17.186s\n",
      "[9600] loss: 1.712, time: 17.194s\n",
      "[9700] loss: 1.741, time: 17.176s\n",
      "[9800] loss: 1.765, time: 17.184s\n",
      "[9900] loss: 1.720, time: 17.185s\n",
      "[10000] loss: 1.687, time: 17.186s\n",
      "train loss: 1.7837, val loss: 1.7831\n",
      "[10100] loss: 1.673, time: 17.697s\n",
      "[10200] loss: 1.770, time: 17.186s\n",
      "[10300] loss: 1.781, time: 17.187s\n",
      "[10400] loss: 1.640, time: 17.163s\n",
      "[10500] loss: 1.824, time: 17.160s\n",
      "[10600] loss: 1.653, time: 17.211s\n",
      "[10700] loss: 1.770, time: 17.190s\n",
      "[10800] loss: 1.776, time: 17.161s\n",
      "[10900] loss: 1.898, time: 17.169s\n",
      "[11000] loss: 1.843, time: 17.167s\n",
      "train loss: 1.7346, val loss: 1.7524\n",
      "[11100] loss: 1.747, time: 17.715s\n",
      "[11200] loss: 1.778, time: 17.182s\n",
      "[11300] loss: 1.697, time: 17.176s\n",
      "[11400] loss: 1.777, time: 17.195s\n",
      "[11500] loss: 1.745, time: 17.202s\n",
      "[11600] loss: 1.641, time: 17.176s\n",
      "[11700] loss: 1.633, time: 17.174s\n",
      "[11800] loss: 1.687, time: 17.161s\n",
      "[11900] loss: 1.717, time: 17.183s\n",
      "[12000] loss: 1.685, time: 17.174s\n",
      "train loss: 1.7158, val loss: 1.7510\n",
      "[12100] loss: 1.747, time: 17.699s\n",
      "[12200] loss: 1.711, time: 17.173s\n",
      "[12300] loss: 1.614, time: 17.174s\n",
      "[12400] loss: 1.622, time: 17.165s\n",
      "[12500] loss: 1.769, time: 17.167s\n",
      "[12600] loss: 1.845, time: 17.165s\n",
      "[12700] loss: 1.703, time: 17.165s\n",
      "[12800] loss: 1.605, time: 17.183s\n",
      "[12900] loss: 1.657, time: 17.165s\n",
      "[13000] loss: 1.683, time: 17.175s\n",
      "train loss: 1.6706, val loss: 1.6672\n",
      "[13100] loss: 1.638, time: 17.695s\n",
      "[13200] loss: 1.677, time: 17.195s\n",
      "[13300] loss: 1.655, time: 17.197s\n",
      "[13400] loss: 1.702, time: 17.174s\n",
      "[13500] loss: 1.655, time: 17.188s\n",
      "[13600] loss: 1.651, time: 17.190s\n",
      "[13700] loss: 1.709, time: 17.175s\n",
      "[13800] loss: 1.623, time: 17.182s\n",
      "[13900] loss: 1.671, time: 17.164s\n",
      "[14000] loss: 1.650, time: 17.180s\n",
      "train loss: 1.6279, val loss: 1.6905\n",
      "[14100] loss: 1.637, time: 17.520s\n",
      "[14200] loss: 1.725, time: 17.168s\n",
      "[14300] loss: 1.787, time: 17.178s\n",
      "[14400] loss: 1.727, time: 17.185s\n",
      "[14500] loss: 1.521, time: 17.156s\n",
      "[14600] loss: 1.721, time: 17.164s\n",
      "[14700] loss: 1.580, time: 17.164s\n",
      "[14800] loss: 1.555, time: 17.195s\n",
      "[14900] loss: 1.561, time: 17.196s\n",
      "[15000] loss: 1.601, time: 17.201s\n",
      "train loss: 1.5997, val loss: 1.6533\n",
      "[15100] loss: 1.707, time: 17.687s\n",
      "[15200] loss: 1.654, time: 17.188s\n",
      "[15300] loss: 1.522, time: 17.178s\n",
      "[15400] loss: 1.581, time: 17.181s\n",
      "[15500] loss: 1.631, time: 17.168s\n",
      "[15600] loss: 1.627, time: 17.203s\n",
      "[15700] loss: 1.554, time: 17.196s\n",
      "[15800] loss: 1.643, time: 17.177s\n",
      "[15900] loss: 1.556, time: 17.182s\n",
      "[16000] loss: 1.457, time: 17.167s\n",
      "train loss: 1.5602, val loss: 1.6080\n",
      "[16100] loss: 1.503, time: 17.661s\n",
      "[16200] loss: 1.574, time: 17.182s\n",
      "[16300] loss: 1.514, time: 17.165s\n",
      "[16400] loss: 1.624, time: 17.153s\n",
      "[16500] loss: 1.616, time: 17.177s\n",
      "[16600] loss: 1.462, time: 17.195s\n",
      "[16700] loss: 1.503, time: 17.194s\n",
      "[16800] loss: 1.628, time: 17.201s\n",
      "[16900] loss: 1.537, time: 17.179s\n",
      "[17000] loss: 1.491, time: 17.193s\n",
      "train loss: 1.5341, val loss: 1.5832\n",
      "[17100] loss: 1.526, time: 17.692s\n",
      "[17200] loss: 1.394, time: 17.189s\n",
      "[17300] loss: 1.541, time: 17.186s\n",
      "[17400] loss: 1.523, time: 17.171s\n",
      "[17500] loss: 1.452, time: 17.171s\n",
      "[17600] loss: 1.394, time: 17.171s\n",
      "[17700] loss: 1.399, time: 17.179s\n",
      "[17800] loss: 1.439, time: 17.187s\n",
      "[17900] loss: 1.565, time: 17.168s\n",
      "[18000] loss: 1.417, time: 17.164s\n",
      "train loss: 1.4048, val loss: 1.5270\n",
      "[18100] loss: 1.502, time: 17.664s\n",
      "[18200] loss: 1.401, time: 17.191s\n",
      "[18300] loss: 1.530, time: 17.160s\n",
      "[18400] loss: 1.475, time: 17.169s\n",
      "[18500] loss: 1.336, time: 17.180s\n",
      "[18600] loss: 1.460, time: 17.174s\n",
      "[18700] loss: 1.382, time: 17.170s\n",
      "[18800] loss: 1.375, time: 17.182s\n",
      "[18900] loss: 1.239, time: 17.197s\n",
      "[19000] loss: 1.359, time: 17.186s\n",
      "train loss: 1.3227, val loss: 1.3712\n",
      "[19100] loss: 1.268, time: 17.698s\n",
      "[19200] loss: 1.369, time: 17.184s\n",
      "[19300] loss: 1.369, time: 17.168s\n",
      "[19400] loss: 1.308, time: 17.187s\n",
      "[19500] loss: 1.307, time: 17.185s\n",
      "[19600] loss: 1.285, time: 17.185s\n",
      "[19700] loss: 1.423, time: 17.167s\n",
      "[19800] loss: 1.239, time: 17.188s\n",
      "[19900] loss: 1.239, time: 17.150s\n",
      "[20000] loss: 1.296, time: 17.164s\n",
      "train loss: 1.2107, val loss: 1.2688\n",
      "[20100] loss: 1.169, time: 17.692s\n",
      "[20200] loss: 1.211, time: 17.178s\n",
      "[20300] loss: 1.257, time: 17.200s\n",
      "[20400] loss: 1.255, time: 17.177s\n",
      "[20500] loss: 1.161, time: 17.193s\n",
      "[20600] loss: 1.224, time: 17.173s\n",
      "[20700] loss: 1.089, time: 17.173s\n",
      "[20800] loss: 1.066, time: 17.184s\n",
      "[20900] loss: 1.140, time: 17.172s\n",
      "[21000] loss: 1.034, time: 17.167s\n",
      "train loss: 1.0732, val loss: 1.1513\n",
      "[21100] loss: 1.083, time: 17.678s\n",
      "[21200] loss: 1.169, time: 17.191s\n",
      "[21300] loss: 1.103, time: 17.196s\n",
      "[21400] loss: 1.066, time: 17.184s\n",
      "[21500] loss: 1.014, time: 17.175s\n",
      "[21600] loss: 1.014, time: 17.179s\n",
      "[21700] loss: 1.215, time: 17.172s\n",
      "[21800] loss: 1.107, time: 17.184s\n",
      "[21900] loss: 0.908, time: 17.212s\n",
      "[22000] loss: 0.950, time: 17.185s\n",
      "train loss: 0.8866, val loss: 1.0132\n",
      "[22100] loss: 0.910, time: 17.698s\n",
      "[22200] loss: 0.867, time: 17.170s\n",
      "[22300] loss: 1.005, time: 17.167s\n",
      "[22400] loss: 0.844, time: 17.177s\n",
      "[22500] loss: 0.786, time: 17.179s\n",
      "[22600] loss: 0.793, time: 17.188s\n",
      "[22700] loss: 0.916, time: 17.212s\n",
      "[22800] loss: 0.866, time: 17.190s\n",
      "[22900] loss: 0.900, time: 17.186s\n",
      "[23000] loss: 0.790, time: 17.171s\n",
      "train loss: 0.9005, val loss: 0.9362\n",
      "[23100] loss: 0.837, time: 17.696s\n",
      "[23200] loss: 0.836, time: 17.175s\n",
      "[23300] loss: 0.933, time: 17.174s\n",
      "[23400] loss: 0.807, time: 17.148s\n",
      "[23500] loss: 0.703, time: 17.161s\n",
      "[23600] loss: 0.802, time: 17.175s\n",
      "[23700] loss: 0.854, time: 17.170s\n",
      "[23800] loss: 0.674, time: 17.180s\n",
      "[23900] loss: 0.765, time: 17.188s\n",
      "[24000] loss: 0.767, time: 17.186s\n",
      "train loss: 0.7618, val loss: 0.7684\n",
      "[24100] loss: 0.706, time: 17.686s\n",
      "[24200] loss: 0.926, time: 17.189s\n",
      "[24300] loss: 0.833, time: 17.173s\n",
      "[24400] loss: 0.704, time: 17.178s\n",
      "[24500] loss: 0.621, time: 17.179s\n",
      "[24600] loss: 0.616, time: 17.173s\n",
      "[24700] loss: 0.791, time: 17.161s\n",
      "[24800] loss: 0.739, time: 17.169s\n",
      "[24900] loss: 0.701, time: 17.190s\n",
      "[25000] loss: 0.656, time: 17.194s\n",
      "train loss: 0.7296, val loss: 0.7184\n",
      "[25100] loss: 0.701, time: 17.672s\n",
      "[25200] loss: 0.677, time: 17.181s\n",
      "[25300] loss: 0.667, time: 17.159s\n",
      "[25400] loss: 0.685, time: 17.165s\n",
      "[25500] loss: 0.726, time: 17.182s\n",
      "[25600] loss: 0.705, time: 17.167s\n",
      "[25700] loss: 0.597, time: 17.202s\n",
      "[25800] loss: 0.658, time: 17.196s\n",
      "[25900] loss: 0.629, time: 17.180s\n",
      "[26000] loss: 0.731, time: 17.196s\n",
      "train loss: 0.6422, val loss: 0.6839\n",
      "[26100] loss: 0.671, time: 17.678s\n",
      "[26200] loss: 0.585, time: 17.171s\n",
      "[26300] loss: 0.554, time: 17.164s\n",
      "[26400] loss: 0.523, time: 17.200s\n",
      "[26500] loss: 0.685, time: 17.174s\n",
      "[26600] loss: 0.599, time: 17.175s\n",
      "[26700] loss: 0.505, time: 17.177s\n",
      "[26800] loss: 0.541, time: 17.168s\n",
      "[26900] loss: 0.745, time: 17.158s\n",
      "[27000] loss: 0.643, time: 17.157s\n",
      "train loss: 0.6283, val loss: 0.5977\n",
      "[27100] loss: 0.498, time: 17.673s\n",
      "[27200] loss: 0.500, time: 17.193s\n",
      "[27300] loss: 0.554, time: 17.160s\n",
      "[27400] loss: 0.467, time: 17.184s\n",
      "[27500] loss: 0.582, time: 17.181s\n",
      "[27600] loss: 0.545, time: 17.193s\n",
      "[27700] loss: 0.511, time: 17.175s\n",
      "[27800] loss: 0.462, time: 17.165s\n",
      "[27900] loss: 0.522, time: 17.179s\n",
      "[28000] loss: 0.507, time: 17.154s\n",
      "train loss: 0.5760, val loss: 0.5860\n",
      "[28100] loss: 0.509, time: 17.678s\n",
      "[28200] loss: 0.591, time: 17.175s\n",
      "[28300] loss: 0.495, time: 17.152s\n",
      "[28400] loss: 0.482, time: 17.170s\n",
      "[28500] loss: 0.518, time: 17.185s\n",
      "[28600] loss: 0.443, time: 17.168s\n",
      "[28700] loss: 0.509, time: 17.161s\n",
      "[28800] loss: 0.481, time: 17.180s\n",
      "[28900] loss: 0.532, time: 17.171s\n",
      "[29000] loss: 0.540, time: 17.179s\n",
      "train loss: 0.4934, val loss: 0.5480\n",
      "[29100] loss: 0.444, time: 17.690s\n",
      "[29200] loss: 0.574, time: 17.208s\n",
      "[29300] loss: 0.499, time: 17.198s\n",
      "[29400] loss: 0.378, time: 17.167s\n",
      "[29500] loss: 0.391, time: 17.170s\n",
      "[29600] loss: 0.482, time: 17.178s\n",
      "[29700] loss: 0.518, time: 17.183s\n",
      "[29800] loss: 0.412, time: 17.172s\n",
      "[29900] loss: 0.381, time: 17.177s\n",
      "[30000] loss: 0.467, time: 17.176s\n",
      "train loss: 0.5615, val loss: 0.5275\n",
      "[30100] loss: 0.445, time: 17.702s\n",
      "[30200] loss: 0.391, time: 17.290s\n",
      "[30300] loss: 0.454, time: 17.254s\n",
      "[30400] loss: 0.490, time: 17.236s\n",
      "[30500] loss: 0.463, time: 17.171s\n",
      "[30600] loss: 0.414, time: 17.228s\n",
      "[30700] loss: 0.449, time: 17.177s\n",
      "[30800] loss: 0.425, time: 17.175s\n",
      "[30900] loss: 0.384, time: 17.226s\n",
      "[31000] loss: 0.565, time: 17.191s\n",
      "train loss: 0.3463, val loss: 0.4650\n",
      "[31100] loss: 0.435, time: 17.703s\n",
      "[31200] loss: 0.320, time: 17.189s\n",
      "[31300] loss: 0.550, time: 17.177s\n",
      "[31400] loss: 0.405, time: 17.241s\n",
      "[31500] loss: 0.419, time: 17.209s\n",
      "[31600] loss: 0.299, time: 17.169s\n",
      "[31700] loss: 0.559, time: 17.189s\n",
      "[31800] loss: 0.320, time: 17.176s\n",
      "[31900] loss: 0.396, time: 17.196s\n",
      "[32000] loss: 0.281, time: 17.179s\n",
      "train loss: 0.4206, val loss: 0.4453\n",
      "[32100] loss: 0.311, time: 17.750s\n",
      "[32200] loss: 0.349, time: 17.180s\n",
      "[32300] loss: 0.419, time: 17.152s\n",
      "[32400] loss: 0.332, time: 17.145s\n",
      "[32500] loss: 0.374, time: 17.171s\n",
      "[32600] loss: 0.261, time: 17.166s\n",
      "[32700] loss: 0.383, time: 17.184s\n",
      "[32800] loss: 0.339, time: 17.219s\n",
      "[32900] loss: 0.334, time: 17.224s\n",
      "[33000] loss: 0.369, time: 17.180s\n",
      "train loss: 0.3227, val loss: 0.4060\n",
      "[33100] loss: 0.347, time: 17.692s\n",
      "[33200] loss: 0.313, time: 17.171s\n",
      "[33300] loss: 0.419, time: 17.177s\n",
      "[33400] loss: 0.284, time: 17.166s\n",
      "[33500] loss: 0.265, time: 17.180s\n",
      "[33600] loss: 0.355, time: 17.183s\n",
      "[33700] loss: 0.371, time: 17.237s\n",
      "[33800] loss: 0.286, time: 17.183s\n",
      "[33900] loss: 0.224, time: 17.175s\n",
      "[34000] loss: 0.334, time: 17.229s\n",
      "train loss: 0.3058, val loss: 0.3804\n",
      "[34100] loss: 0.338, time: 17.686s\n",
      "[34200] loss: 0.330, time: 17.233s\n",
      "[34300] loss: 0.286, time: 17.197s\n",
      "[34400] loss: 0.263, time: 17.178s\n",
      "[34500] loss: 0.317, time: 17.188s\n",
      "[34600] loss: 0.194, time: 17.181s\n",
      "[34700] loss: 0.195, time: 17.186s\n",
      "[34800] loss: 0.336, time: 17.169s\n",
      "[34900] loss: 0.375, time: 17.250s\n",
      "[35000] loss: 0.277, time: 17.199s\n",
      "train loss: 0.2948, val loss: 0.2860\n",
      "[35100] loss: 0.364, time: 17.684s\n",
      "[35200] loss: 0.262, time: 17.163s\n",
      "[35300] loss: 0.174, time: 17.197s\n",
      "[35400] loss: 0.294, time: 17.155s\n",
      "[35500] loss: 0.193, time: 17.190s\n",
      "[35600] loss: 0.177, time: 17.209s\n",
      "[35700] loss: 0.233, time: 17.197s\n",
      "[35800] loss: 0.189, time: 17.166s\n",
      "[35900] loss: 0.354, time: 17.166s\n",
      "[36000] loss: 0.280, time: 17.160s\n",
      "train loss: 0.2149, val loss: 0.2920\n",
      "[36100] loss: 0.232, time: 17.506s\n",
      "[36200] loss: 0.147, time: 17.194s\n",
      "[36300] loss: 0.191, time: 17.253s\n",
      "[36400] loss: 0.268, time: 17.204s\n",
      "[36500] loss: 0.235, time: 17.160s\n",
      "[36600] loss: 0.270, time: 17.186s\n",
      "[36700] loss: 0.195, time: 17.170s\n",
      "[36800] loss: 0.137, time: 17.164s\n",
      "[36900] loss: 0.185, time: 17.157s\n",
      "[37000] loss: 0.264, time: 17.213s\n",
      "train loss: 0.2447, val loss: 0.2495\n",
      "[37100] loss: 0.233, time: 17.665s\n",
      "[37200] loss: 0.216, time: 17.154s\n",
      "[37300] loss: 0.258, time: 17.163s\n",
      "[37400] loss: 0.216, time: 17.163s\n",
      "[37500] loss: 0.159, time: 17.164s\n",
      "[37600] loss: 0.357, time: 17.153s\n",
      "[37700] loss: 0.264, time: 17.171s\n",
      "[37800] loss: 0.171, time: 17.177s\n",
      "[37900] loss: 0.199, time: 17.169s\n",
      "[38000] loss: 0.270, time: 17.179s\n",
      "train loss: 0.1865, val loss: 0.2529\n",
      "[38100] loss: 0.185, time: 17.526s\n",
      "[38200] loss: 0.313, time: 17.198s\n",
      "[38300] loss: 0.192, time: 17.179s\n",
      "[38400] loss: 0.216, time: 17.166s\n",
      "[38500] loss: 0.163, time: 17.165s\n",
      "[38600] loss: 0.181, time: 17.170s\n",
      "[38700] loss: 0.266, time: 17.189s\n",
      "[38800] loss: 0.210, time: 17.187s\n",
      "[38900] loss: 0.146, time: 17.177s\n",
      "[39000] loss: 0.151, time: 17.179s\n",
      "train loss: 0.1767, val loss: 0.2208\n",
      "[39100] loss: 0.206, time: 17.673s\n",
      "[39200] loss: 0.134, time: 17.171s\n",
      "[39300] loss: 0.180, time: 17.163s\n",
      "[39400] loss: 0.217, time: 17.120s\n",
      "[39500] loss: 0.230, time: 17.173s\n",
      "[39600] loss: 0.176, time: 17.158s\n",
      "[39700] loss: 0.184, time: 17.174s\n",
      "[39800] loss: 0.231, time: 17.183s\n",
      "[39900] loss: 0.162, time: 17.180s\n",
      "[40000] loss: 0.138, time: 17.155s\n",
      "train loss: 0.1790, val loss: 0.2297\n",
      "[40100] loss: 0.163, time: 17.507s\n",
      "[40200] loss: 0.132, time: 17.184s\n",
      "[40300] loss: 0.224, time: 17.171s\n",
      "[40400] loss: 0.199, time: 17.168s\n",
      "[40500] loss: 0.209, time: 17.170s\n",
      "[40600] loss: 0.139, time: 17.165s\n",
      "[40700] loss: 0.218, time: 17.223s\n",
      "[40800] loss: 0.282, time: 17.169s\n",
      "[40900] loss: 0.170, time: 17.182s\n",
      "[41000] loss: 0.132, time: 17.161s\n",
      "train loss: 0.1383, val loss: 0.2417\n",
      "[41100] loss: 0.208, time: 17.462s\n",
      "[41200] loss: 0.209, time: 17.149s\n",
      "[41300] loss: 0.099, time: 17.143s\n",
      "[41400] loss: 0.146, time: 17.157s\n",
      "[41500] loss: 0.210, time: 17.144s\n",
      "[41600] loss: 0.149, time: 17.163s\n",
      "[41700] loss: 0.176, time: 17.167s\n",
      "[41800] loss: 0.097, time: 17.148s\n",
      "[41900] loss: 0.185, time: 17.151s\n",
      "[42000] loss: 0.110, time: 17.170s\n",
      "train loss: 0.1179, val loss: 0.1666\n",
      "[42100] loss: 0.133, time: 17.655s\n",
      "[42200] loss: 0.146, time: 17.163s\n",
      "[42300] loss: 0.163, time: 17.156s\n",
      "[42400] loss: 0.189, time: 17.198s\n",
      "[42500] loss: 0.119, time: 17.160s\n",
      "[42600] loss: 0.154, time: 17.150s\n",
      "[42700] loss: 0.128, time: 17.157s\n",
      "[42800] loss: 0.147, time: 17.153s\n",
      "[42900] loss: 0.129, time: 17.154s\n",
      "[43000] loss: 0.128, time: 17.139s\n",
      "train loss: 0.1224, val loss: 0.1509\n",
      "[43100] loss: 0.166, time: 17.660s\n",
      "[43200] loss: 0.134, time: 17.168s\n",
      "[43300] loss: 0.139, time: 17.120s\n",
      "[43400] loss: 0.126, time: 17.170s\n",
      "[43500] loss: 0.116, time: 17.163s\n",
      "[43600] loss: 0.136, time: 17.161s\n",
      "[43700] loss: 0.127, time: 17.188s\n",
      "[43800] loss: 0.172, time: 17.151s\n",
      "[43900] loss: 0.131, time: 17.163s\n",
      "[44000] loss: 0.107, time: 17.196s\n",
      "train loss: 0.1170, val loss: 0.1472\n",
      "[44100] loss: 0.117, time: 17.749s\n",
      "[44200] loss: 0.130, time: 17.204s\n",
      "[44300] loss: 0.109, time: 17.186s\n",
      "[44400] loss: 0.096, time: 17.192s\n",
      "[44500] loss: 0.101, time: 17.173s\n",
      "[44600] loss: 0.108, time: 17.155s\n",
      "[44700] loss: 0.122, time: 17.166s\n",
      "[44800] loss: 0.144, time: 17.154s\n",
      "[44900] loss: 0.100, time: 17.164s\n",
      "[45000] loss: 0.135, time: 17.173s\n",
      "train loss: 0.1226, val loss: 0.1274\n",
      "[45100] loss: 0.103, time: 17.693s\n",
      "[45200] loss: 0.109, time: 17.207s\n",
      "[45300] loss: 0.109, time: 17.198s\n",
      "[45400] loss: 0.102, time: 17.178s\n",
      "[45500] loss: 0.129, time: 17.162s\n",
      "[45600] loss: 0.117, time: 17.144s\n",
      "[45700] loss: 0.115, time: 17.180s\n",
      "[45800] loss: 0.135, time: 17.154s\n",
      "[45900] loss: 0.081, time: 17.160s\n",
      "[46000] loss: 0.162, time: 17.151s\n",
      "train loss: 0.0952, val loss: 0.1110\n",
      "[46100] loss: 0.084, time: 17.653s\n",
      "[46200] loss: 0.103, time: 17.157s\n",
      "[46300] loss: 0.100, time: 17.166s\n",
      "[46400] loss: 0.102, time: 17.165s\n",
      "[46500] loss: 0.069, time: 17.153s\n",
      "[46600] loss: 0.117, time: 17.137s\n",
      "[46700] loss: 0.126, time: 17.163s\n",
      "[46800] loss: 0.104, time: 17.154s\n",
      "[46900] loss: 0.085, time: 17.166s\n",
      "[47000] loss: 0.133, time: 17.179s\n",
      "train loss: 0.1072, val loss: 0.1163\n",
      "[47100] loss: 0.091, time: 17.515s\n",
      "[47200] loss: 0.087, time: 17.182s\n",
      "[47300] loss: 0.148, time: 17.164s\n",
      "[47400] loss: 0.095, time: 17.143s\n",
      "[47500] loss: 0.097, time: 17.151s\n",
      "[47600] loss: 0.070, time: 17.163s\n",
      "[47700] loss: 0.093, time: 17.170s\n",
      "[47800] loss: 0.111, time: 17.146s\n",
      "[47900] loss: 0.087, time: 17.166s\n",
      "[48000] loss: 0.114, time: 17.166s\n",
      "train loss: 0.0969, val loss: 0.0985\n",
      "[48100] loss: 0.098, time: 17.683s\n",
      "[48200] loss: 0.077, time: 17.150s\n",
      "[48300] loss: 0.081, time: 17.148s\n",
      "[48400] loss: 0.105, time: 17.175s\n",
      "[48500] loss: 0.120, time: 17.180s\n",
      "[48600] loss: 0.093, time: 17.129s\n",
      "[48700] loss: 0.112, time: 17.151s\n",
      "[48800] loss: 0.104, time: 17.184s\n",
      "[48900] loss: 0.116, time: 17.168s\n",
      "[49000] loss: 0.093, time: 17.180s\n",
      "train loss: 0.0949, val loss: 0.1130\n",
      "[49100] loss: 0.090, time: 17.498s\n",
      "[49200] loss: 0.105, time: 17.165s\n",
      "[49300] loss: 0.080, time: 17.167s\n",
      "[49400] loss: 0.099, time: 17.156s\n",
      "[49500] loss: 0.087, time: 17.153s\n",
      "[49600] loss: 0.087, time: 17.170s\n",
      "[49700] loss: 0.086, time: 17.150s\n",
      "[49800] loss: 0.114, time: 17.163s\n",
      "[49900] loss: 0.091, time: 17.150s\n",
      "[50000] loss: 0.080, time: 17.159s\n",
      "train loss: 0.0854, val loss: 0.0972\n",
      "[50100] loss: 0.076, time: 17.643s\n",
      "[50200] loss: 0.090, time: 17.142s\n",
      "[50300] loss: 0.066, time: 17.156s\n",
      "[50400] loss: 0.063, time: 17.162s\n",
      "[50500] loss: 0.092, time: 17.114s\n",
      "[50600] loss: 0.070, time: 17.163s\n",
      "[50700] loss: 0.088, time: 17.189s\n",
      "[50800] loss: 0.078, time: 17.177s\n",
      "[50900] loss: 0.069, time: 17.165s\n",
      "[51000] loss: 0.103, time: 17.149s\n",
      "train loss: 0.0874, val loss: 0.0998\n",
      "[51100] loss: 0.098, time: 17.494s\n",
      "[51200] loss: 0.086, time: 17.180s\n",
      "[51300] loss: 0.065, time: 17.158s\n",
      "[51400] loss: 0.080, time: 17.147s\n",
      "[51500] loss: 0.095, time: 17.167s\n",
      "[51600] loss: 0.085, time: 17.160s\n",
      "[51700] loss: 0.129, time: 17.169s\n",
      "[51800] loss: 0.109, time: 17.158s\n",
      "[51900] loss: 0.105, time: 17.144s\n",
      "[52000] loss: 0.084, time: 17.139s\n",
      "train loss: 0.0909, val loss: 0.0872\n",
      "[52100] loss: 0.082, time: 17.681s\n",
      "[52200] loss: 0.106, time: 17.158s\n",
      "[52300] loss: 0.088, time: 17.186s\n",
      "[52400] loss: 0.099, time: 17.168s\n",
      "[52500] loss: 0.072, time: 17.166s\n",
      "[52600] loss: 0.083, time: 17.160s\n",
      "[52700] loss: 0.091, time: 17.183s\n",
      "[52800] loss: 0.093, time: 17.155s\n",
      "[52900] loss: 0.068, time: 17.157s\n",
      "[53000] loss: 0.088, time: 17.164s\n",
      "train loss: 0.0858, val loss: 0.0897\n",
      "[53100] loss: 0.064, time: 17.488s\n",
      "[53200] loss: 0.080, time: 17.154s\n",
      "[53300] loss: 0.091, time: 17.163s\n",
      "[53400] loss: 0.064, time: 17.182s\n",
      "[53500] loss: 0.074, time: 17.147s\n",
      "[53600] loss: 0.087, time: 17.155s\n",
      "[53700] loss: 0.101, time: 17.138s\n",
      "[53800] loss: 0.074, time: 17.142s\n",
      "[53900] loss: 0.116, time: 17.151s\n",
      "[54000] loss: 0.087, time: 17.168s\n",
      "train loss: 0.0784, val loss: 0.0843\n",
      "[54100] loss: 0.088, time: 17.683s\n",
      "[54200] loss: 0.081, time: 17.173s\n",
      "[54300] loss: 0.098, time: 17.169s\n",
      "[54400] loss: 0.069, time: 17.170s\n",
      "[54500] loss: 0.073, time: 17.161s\n",
      "[54600] loss: 0.082, time: 17.162s\n",
      "[54700] loss: 0.076, time: 17.149s\n",
      "[54800] loss: 0.064, time: 17.161s\n",
      "[54900] loss: 0.094, time: 17.152s\n",
      "[55000] loss: 0.086, time: 17.165s\n",
      "train loss: 0.0719, val loss: 0.0940\n",
      "[55100] loss: 0.082, time: 17.491s\n",
      "[55200] loss: 0.107, time: 17.168s\n",
      "[55300] loss: 0.091, time: 17.161s\n",
      "[55400] loss: 0.069, time: 17.150s\n",
      "[55500] loss: 0.082, time: 17.133s\n",
      "[55600] loss: 0.074, time: 17.127s\n",
      "[55700] loss: 0.085, time: 17.169s\n",
      "[55800] loss: 0.083, time: 17.170s\n",
      "[55900] loss: 0.076, time: 17.168s\n",
      "[56000] loss: 0.072, time: 17.183s\n",
      "train loss: 0.0678, val loss: 0.0855\n",
      "[56100] loss: 0.082, time: 17.504s\n",
      "[56200] loss: 0.123, time: 17.185s\n",
      "[56300] loss: 0.080, time: 17.172s\n",
      "[56400] loss: 0.095, time: 17.160s\n",
      "[56500] loss: 0.072, time: 17.153s\n",
      "[56600] loss: 0.076, time: 17.150s\n",
      "[56700] loss: 0.086, time: 17.165s\n",
      "[56800] loss: 0.106, time: 17.182s\n",
      "[56900] loss: 0.090, time: 17.161s\n",
      "[57000] loss: 0.073, time: 17.156s\n",
      "train loss: 0.0814, val loss: 0.0914\n",
      "[57100] loss: 0.079, time: 17.504s\n",
      "[57200] loss: 0.058, time: 17.155s\n",
      "[57300] loss: 0.083, time: 17.138s\n",
      "[57400] loss: 0.095, time: 17.157s\n",
      "[57500] loss: 0.073, time: 17.180s\n",
      "[57600] loss: 0.078, time: 17.192s\n",
      "[57700] loss: 0.078, time: 17.174s\n",
      "[57800] loss: 0.103, time: 17.167s\n",
      "[57900] loss: 0.074, time: 17.164s\n",
      "[58000] loss: 0.074, time: 17.162s\n",
      "train loss: 0.0806, val loss: 0.0778\n",
      "[58100] loss: 0.063, time: 17.692s\n",
      "[58200] loss: 0.096, time: 17.159s\n",
      "[58300] loss: 0.063, time: 17.169s\n",
      "[58400] loss: 0.062, time: 17.171s\n",
      "[58500] loss: 0.076, time: 17.164s\n",
      "[58600] loss: 0.088, time: 17.152s\n",
      "[58700] loss: 0.081, time: 17.157s\n",
      "[58800] loss: 0.095, time: 17.173s\n",
      "[58900] loss: 0.055, time: 17.163s\n",
      "[59000] loss: 0.081, time: 17.152s\n",
      "train loss: 0.0775, val loss: 0.0709\n",
      "[59100] loss: 0.080, time: 17.665s\n",
      "[59200] loss: 0.081, time: 17.152s\n",
      "[59300] loss: 0.072, time: 17.187s\n",
      "[59400] loss: 0.076, time: 17.165s\n",
      "[59500] loss: 0.067, time: 17.185s\n",
      "[59600] loss: 0.087, time: 17.165s\n",
      "[59700] loss: 0.073, time: 17.174s\n",
      "[59800] loss: 0.070, time: 17.166s\n",
      "[59900] loss: 0.078, time: 17.155s\n",
      "[60000] loss: 0.064, time: 17.167s\n",
      "train loss: 0.0760, val loss: 0.0834\n",
      "[60100] loss: 0.081, time: 17.504s\n",
      "[60200] loss: 0.062, time: 17.154s\n",
      "[60300] loss: 0.059, time: 17.174s\n",
      "[60400] loss: 0.061, time: 17.175s\n",
      "[60500] loss: 0.068, time: 17.170s\n",
      "[60600] loss: 0.073, time: 17.158s\n",
      "[60700] loss: 0.070, time: 17.165s\n",
      "[60800] loss: 0.108, time: 17.156s\n",
      "[60900] loss: 0.056, time: 17.159s\n",
      "[61000] loss: 0.075, time: 17.175s\n",
      "train loss: 0.0710, val loss: 0.0778\n",
      "[61100] loss: 0.077, time: 17.506s\n",
      "[61200] loss: 0.079, time: 17.171s\n",
      "[61300] loss: 0.078, time: 17.179s\n",
      "[61400] loss: 0.061, time: 17.174s\n",
      "[61500] loss: 0.089, time: 17.146s\n",
      "[61600] loss: 0.070, time: 17.151s\n",
      "[61700] loss: 0.063, time: 17.174s\n",
      "[61800] loss: 0.065, time: 17.164s\n",
      "[61900] loss: 0.059, time: 17.164s\n",
      "[62000] loss: 0.066, time: 17.166s\n",
      "train loss: 0.0665, val loss: 0.0906\n",
      "[62100] loss: 0.067, time: 17.497s\n",
      "[62200] loss: 0.105, time: 17.164s\n",
      "[62300] loss: 0.078, time: 17.152s\n",
      "[62400] loss: 0.069, time: 17.145s\n",
      "[62500] loss: 0.062, time: 17.154s\n",
      "[62600] loss: 0.083, time: 17.157s\n",
      "[62700] loss: 0.065, time: 17.155s\n",
      "[62800] loss: 0.066, time: 17.166s\n",
      "[62900] loss: 0.058, time: 47.895s\n",
      "[63000] loss: 0.070, time: 16.808s\n",
      "train loss: 0.0741, val loss: 0.0684\n",
      "[63100] loss: 0.065, time: 284.142s\n",
      "[63200] loss: 0.081, time: 16.716s\n",
      "[63300] loss: 0.066, time: 16.744s\n",
      "[63400] loss: 0.083, time: 915.161s\n",
      "[63500] loss: 0.070, time: 16.697s\n",
      "[63600] loss: 0.080, time: 16.711s\n",
      "[63700] loss: 0.093, time: 636.085s\n",
      "[63800] loss: 0.070, time: 16.689s\n",
      "[63900] loss: 0.083, time: 16.734s\n",
      "[64000] loss: 0.058, time: 16.783s\n",
      "train loss: 0.0674, val loss: 0.0779\n",
      "[64100] loss: 0.070, time: 17.142s\n",
      "[64200] loss: 0.069, time: 16.778s\n",
      "[64300] loss: 0.069, time: 16.801s\n",
      "[64400] loss: 0.081, time: 17.096s\n",
      "[64500] loss: 0.096, time: 18.191s\n",
      "[64600] loss: 0.076, time: 642.062s\n",
      "[64700] loss: 0.066, time: 16.816s\n",
      "[64800] loss: 0.090, time: 16.716s\n",
      "[64900] loss: 0.055, time: 16.731s\n",
      "[65000] loss: 0.071, time: 16.759s\n",
      "train loss: 0.0687, val loss: 0.0775\n",
      "[65100] loss: 0.083, time: 426.112s\n",
      "[65200] loss: 0.099, time: 16.699s\n",
      "[65300] loss: 0.085, time: 16.726s\n",
      "[65400] loss: 0.067, time: 16.745s\n",
      "[65500] loss: 0.073, time: 16.770s\n",
      "[65600] loss: 0.074, time: 16.800s\n",
      "[65700] loss: 0.094, time: 16.800s\n",
      "[65800] loss: 0.063, time: 17.277s\n",
      "[65900] loss: 0.073, time: 18.397s\n",
      "[66000] loss: 0.061, time: 100.710s\n",
      "train loss: 0.0624, val loss: 0.0707\n",
      "[66100] loss: 0.085, time: 17.081s\n",
      "[66200] loss: 0.070, time: 16.771s\n",
      "[66300] loss: 0.052, time: 509.159s\n",
      "[66400] loss: 0.085, time: 16.699s\n",
      "[66500] loss: 0.088, time: 16.717s\n",
      "[66600] loss: 0.079, time: 915.305s\n",
      "[66700] loss: 0.074, time: 16.839s\n",
      "[66800] loss: 0.079, time: 16.781s\n",
      "[66900] loss: 0.066, time: 998.768s\n",
      "[67000] loss: 0.088, time: 16.717s\n",
      "train loss: 0.0678, val loss: 0.0680\n",
      "[67100] loss: 0.064, time: 17.240s\n",
      "[67200] loss: 0.070, time: 16.724s\n",
      "[67300] loss: 0.073, time: 16.720s\n",
      "[67400] loss: 0.070, time: 16.787s\n",
      "[67500] loss: 0.074, time: 16.790s\n",
      "[67600] loss: 0.052, time: 16.815s\n",
      "[67700] loss: 0.058, time: 17.480s\n",
      "[67800] loss: 0.059, time: 18.479s\n",
      "[67900] loss: 0.110, time: 1020.428s\n",
      "[68000] loss: 0.073, time: 17.001s\n",
      "train loss: 0.0848, val loss: 0.0667\n",
      "[68100] loss: 0.087, time: 17.211s\n",
      "[68200] loss: 0.081, time: 16.742s\n",
      "[68300] loss: 0.064, time: 16.739s\n",
      "[68400] loss: 0.053, time: 16.787s\n",
      "[68500] loss: 0.082, time: 16.779s\n",
      "[68600] loss: 0.074, time: 16.805s\n",
      "[68700] loss: 0.051, time: 377.521s\n",
      "[68800] loss: 0.064, time: 16.691s\n",
      "[68900] loss: 0.060, time: 16.719s\n",
      "[69000] loss: 0.071, time: 927.907s\n",
      "train loss: 0.0742, val loss: 0.0697\n",
      "[69100] loss: 0.095, time: 17.025s\n",
      "[69200] loss: 0.074, time: 16.707s\n",
      "[69300] loss: 0.070, time: 937.077s\n",
      "[69400] loss: 0.069, time: 16.694s\n",
      "[69500] loss: 0.065, time: 16.678s\n",
      "[69600] loss: 0.064, time: 16.740s\n",
      "[69700] loss: 0.072, time: 16.755s\n",
      "[69800] loss: 0.064, time: 16.760s\n",
      "[69900] loss: 0.055, time: 16.785s\n",
      "[70000] loss: 0.070, time: 16.815s\n",
      "train loss: 0.0692, val loss: 0.0753\n",
      "[70100] loss: 0.063, time: 17.351s\n",
      "[70200] loss: 0.079, time: 17.867s\n",
      "[70300] loss: 0.079, time: 942.427s\n",
      "[70400] loss: 0.081, time: 16.692s\n",
      "[70500] loss: 0.057, time: 16.733s\n",
      "[70600] loss: 0.075, time: 16.761s\n",
      "[70700] loss: 0.067, time: 16.784s\n",
      "[70800] loss: 0.072, time: 16.773s\n",
      "[70900] loss: 0.071, time: 16.800s\n",
      "[71000] loss: 0.074, time: 16.798s\n",
      "train loss: 0.0592, val loss: 0.0746\n",
      "[71100] loss: 0.076, time: 17.823s\n",
      "[71200] loss: 0.064, time: 18.456s\n",
      "[71300] loss: 0.095, time: 18.999s\n",
      "[71400] loss: 0.055, time: 147.142s\n",
      "[71500] loss: 0.065, time: 16.722s\n",
      "[71600] loss: 0.070, time: 16.760s\n",
      "[71700] loss: 0.063, time: 290.104s\n",
      "[71800] loss: 0.055, time: 16.723s\n",
      "[71900] loss: 0.070, time: 16.729s\n",
      "[72000] loss: 0.063, time: 241.072s\n",
      "train loss: 0.0671, val loss: 0.0744\n",
      "[72100] loss: 0.059, time: 17.061s\n",
      "[72200] loss: 0.068, time: 940.200s\n",
      "[72300] loss: 0.086, time: 16.687s\n",
      "[72400] loss: 0.074, time: 16.719s\n",
      "[72500] loss: 0.073, time: 16.723s\n",
      "[72600] loss: 0.092, time: 16.737s\n",
      "[72700] loss: 0.081, time: 16.762s\n",
      "[72800] loss: 0.059, time: 16.791s\n",
      "[72900] loss: 0.058, time: 16.821s\n",
      "[73000] loss: 0.072, time: 17.128s\n",
      "train loss: 0.0676, val loss: 0.0788\n",
      "[73100] loss: 0.080, time: 18.472s\n",
      "[73200] loss: 0.063, time: 257.068s\n",
      "[73300] loss: 0.064, time: 16.691s\n",
      "[73400] loss: 0.072, time: 16.734s\n",
      "[73500] loss: 0.078, time: 16.737s\n",
      "[73600] loss: 0.068, time: 16.793s\n",
      "[73700] loss: 0.098, time: 16.813s\n",
      "[73800] loss: 0.060, time: 17.207s\n",
      "[73900] loss: 0.056, time: 22.040s\n",
      "[74000] loss: 0.085, time: 176.394s\n",
      "train loss: 0.0669, val loss: 0.0709\n",
      "[74100] loss: 0.064, time: 17.492s\n",
      "[74200] loss: 0.081, time: 17.407s\n",
      "[74300] loss: 0.060, time: 17.032s\n",
      "[74400] loss: 0.068, time: 17.123s\n",
      "[74500] loss: 0.071, time: 17.078s\n",
      "[74600] loss: 0.076, time: 17.014s\n",
      "[74700] loss: 0.072, time: 17.254s\n",
      "[74800] loss: 0.045, time: 17.397s\n",
      "[74900] loss: 0.060, time: 17.301s\n",
      "[75000] loss: 0.056, time: 16.888s\n",
      "train loss: 0.0606, val loss: 0.0707\n",
      "[75100] loss: 0.071, time: 17.216s\n",
      "[75200] loss: 0.064, time: 16.917s\n",
      "[75300] loss: 0.079, time: 16.851s\n",
      "[75400] loss: 0.060, time: 16.981s\n",
      "[75500] loss: 0.083, time: 17.704s\n",
      "[75600] loss: 0.067, time: 17.323s\n",
      "[75700] loss: 0.057, time: 17.079s\n",
      "[75800] loss: 0.049, time: 17.076s\n",
      "[75900] loss: 0.067, time: 17.063s\n",
      "[76000] loss: 0.063, time: 17.056s\n",
      "train loss: 0.0660, val loss: 0.0707\n",
      "[76100] loss: 0.047, time: 17.399s\n",
      "[76200] loss: 0.086, time: 17.072s\n",
      "[76300] loss: 0.071, time: 17.078s\n",
      "[76400] loss: 0.058, time: 17.075s\n",
      "[76500] loss: 0.073, time: 17.047s\n",
      "[76600] loss: 0.088, time: 17.049s\n",
      "[76700] loss: 0.060, time: 17.090s\n",
      "[76800] loss: 0.057, time: 17.086s\n",
      "[76900] loss: 0.124, time: 17.093s\n",
      "[77000] loss: 0.051, time: 17.093s\n",
      "train loss: 0.0574, val loss: 0.0715\n",
      "[77100] loss: 0.063, time: 17.904s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_step(x, y)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "t0 = time.time()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "while True:\n",
    "    for x, y in batch_iterate(x_train, y_train, batch_size=BATCH_SIZE):\n",
    "        if i >= MAX_ITERS:\n",
    "            break\n",
    "        \n",
    "        if optimizer.learning_rate > 1e-5:\n",
    "            optimizer.learning_rate *= 0.999\n",
    "\n",
    "        loss = train_step(x, y)\n",
    "        mx.eval(state)\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"[{i:4}] loss: {loss.item():.3f}, time: {dt:.3f}s\")\n",
    "        \n",
    "        if i % EVAL_INTERVAL == 0:\n",
    "            train_loss = evaluate_loss(x_train, y_train)\n",
    "            val_loss = evaluate_loss(x_val, y_val)\n",
    "            print(f\"train loss: {train_loss:.4f}, val loss: {val_loss:.4f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                model.save_weights(f\"checkpoints/model1.npz\")   \n",
    "                print(\"saved\")\n",
    "\n",
    "        i += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0674, val loss: 0.0701\n"
     ]
    }
   ],
   "source": [
    "train_loss = evaluate_loss(x_train, y_train)\n",
    "val_loss = evaluate_loss(x_val, y_val)\n",
    "print(f\"train loss: {train_loss:.4f}, val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Ethanioooooooom:\n",
      "Ed, woly, you het'st howit es omen.\n",
      "Now, woul: fas? hout han? Gou Larick, I naved, yor\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "context = mx.array(encode(\"Hello, my name is Ethan\"))[None]\n",
    "output = model.generate(context, max_new_tokens=100, temperature=1)\n",
    "mx.eval(output)\n",
    "print(decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
