{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from typing import Generator, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from mlx import core as mx, nn, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are chars, so the vocab size is the number of unique chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char2int = {c: i for i, c in enumerate(chars)}\n",
    "int2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    return [char2int[c] for c in s if c in char2int]\n",
    "\n",
    "\n",
    "def decode(y: list[int] | np.ndarray | mx.array) -> str:\n",
    "    if isinstance(y, mx.array):\n",
    "        y = np.array(y)\n",
    "    return \"\".join([int2char[int(i)] for i in y if int(i) in int2char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text is encoded as an `mx.array`, then split into training and validation\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = mx.array(encode(text), dtype=mx.int64)\n",
    "\n",
    "val_size = len(full_data) // 10\n",
    "\n",
    "train_data = full_data[val_size:]\n",
    "val_data = full_data[:val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes data into blocks\n",
    "\n",
    "$x_i = [d_i, d_{i + 1}, ..., d_{i + b}]$\n",
    "\n",
    "$y_i = [d_{i + 1}, d_{i + 2}, ..., d_{i + b + 1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_data(data: mx.array, block_size: int) -> tuple[mx.array, mx.array]:\n",
    "    n_blocks = len(data) - block_size - 1\n",
    "    x = mx.stack([data[i:i + block_size] for i in range(n_blocks)])\n",
    "    y = mx.stack([data[i:i + block_size] for i in range(1, n_blocks + 1)])\n",
    "    mx.eval(x, y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates random batches given $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterate(\n",
    "    x: mx.array, \n",
    "    y: mx.array, \n",
    "    batch_size: int,\n",
    ") -> Generator[tuple[mx.array, mx.array], None, None]:\n",
    "    permutation = mx.array(np.random.permutation(y.shape[0]))\n",
    "    for s in range(0, y.shape[0], batch_size):\n",
    "        idxs = permutation[s:s + batch_size]\n",
    "        bx = x[idxs]\n",
    "        by = y[idxs]\n",
    "        mx.eval(bx, by)\n",
    "        yield bx, by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "$y = \\frac{x - E[x]}{\\sqrt{E[(x - E[x])^2] + \\epsilon}} \\odot \\gamma + \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: Sequence[int], bias=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = mx.ones(normalized_shape)\n",
    "        self.bias = mx.ones(normalized_shape) if bias else None\n",
    "        self.eps = eps\n",
    "    \n",
    "\n",
    "    def __call__(self, x: mx.array): \n",
    "        return mx.fast.layer_norm(x, self.weight, self.bias, self.eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "$[q_{i, j}, k_{i, j}, v_{i, j}] = x_{i, j}[W_q, W_k, W_v] + [b_q, b_k, b_v]$ \n",
    "where all $q, k, v, x, b$ are row vectors\n",
    "\n",
    "$[q_{i, j}, k_{i, j}, v_{i, j}]$ are computed for $x_i$ in the 3D tensor \n",
    "$x = \\begin{bmatrix}\n",
    "x_{1, 1} & \\dots & x_{1, T} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{B, 1} & \\dots & x_{B, T}\n",
    "\\end{bmatrix}$ resulting in tensors $q, k, v$\n",
    "\n",
    "$x$ has shape $(B, T, C)$ where $B$ is the batch size, $T$ is the sequence length,\n",
    "and $C$ is the number of embedding dimensions\n",
    "\n",
    "$q, k, v$ have shape $(B, T, ND)$ where $B$ is the batch size, \n",
    "$T$ is the sequence length, $N$ is the number of attention heads, and $D$ is the\n",
    "number of query/key dimensions\n",
    "\n",
    "$q, k, v$ are reshaped to $(B, N, T, D)$\n",
    "\n",
    "$a_{i, j} = q_{i, j} k_{i, j}^T$\n",
    "\n",
    "$a_{i, j} = -\\infty$ for all $i < j$\n",
    "\n",
    "$a_{i, j} = \\text{softmax}(a_{i, j})$ where softmax is computed rowwise\n",
    "\n",
    "$y_{i, j} = a_{i, j} v_{i, j}$\n",
    "\n",
    "$y$ has shape $(B, N, T, D)$\n",
    "\n",
    "$y$ is reshaped to $(B, T, ND)$, so $y_{i, j}$ is a row vector\n",
    "\n",
    "$y_{i, j} = y_{i, j}W_p + b_p$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_head: int, dropout: float, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        assert n_embed % n_head == 0 \n",
    "        self.D = n_embed // n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_scale = 1.0 / np.sqrt(self.D)\n",
    "    \n",
    "\n",
    "    def __call__(self, x: mx.array):\n",
    "        B, T, n_embed = x.shape\n",
    "        assert n_embed == self.n_embed\n",
    "\n",
    "        tmp = self.c_attn(x)\n",
    "        tmp = tmp.split(self.n_embed, axis=2)\n",
    "\n",
    "        q, k, v = mx.split(self.c_attn(x), 3, axis=2)\n",
    "\n",
    "        # reshape to (B, N, T, D)\n",
    "        q = q.reshape((B, T, self.n_head, self.D)).transpose((0, 2, 1, 3)) \n",
    "        k = k.reshape((B, T, self.n_head, self.D)).transpose((0, 2, 1, 3))\n",
    "        v = v.reshape((B, T, self.n_head, self.D)).transpose((0, 2, 1, 3))\n",
    "        \n",
    "        y = mx.fast.scaled_dot_product_attention(q, k, v, scale=self.attn_scale)\n",
    "        y = y.transpose((0, 2, 1, 3)).reshape((B, T, self.n_embed)) # concat head outputs \n",
    "        y = self.c_proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "$x_{i, j} = x_{i, j} W_{c} + B_{c}$\n",
    "\n",
    "$y_{i, j} = x_{i, j} W_{p} + B_{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embed: int, dropout: float, bias=True):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_embed, 4 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(4 * n_embed, n_embed, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def __call__(self, x: mx.array):\n",
    "        x = nn.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block\n",
    "\n",
    "Composition of layer normalization, self attention, and mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_head: int, dropout: float, bias=True):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_embed, bias=bias)\n",
    "        self.attn = SelfAttention(n_embed, n_head, dropout, bias)\n",
    "        self.ln_2 = LayerNorm(n_embed, bias=bias)\n",
    "        self.mlp = MLP(n_embed, dropout, bias)\n",
    "\n",
    "\n",
    "    def __call__(self, x: mx.array):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Transformer\n",
    "- Input is an array of token indexes\n",
    "- Computes token embeddings from the input\n",
    "- Computes position embeddings from the sequence $[0, 1, ..., T - 1]$\n",
    "- $x$ is the sum of the token and position embeddings\n",
    "- $x$ is forwarded through all the blocks\n",
    "- $x$ is layer normalized one more time\n",
    "- $x$ is forwarded through a linear layer to transform it from the embedding dimension \n",
    "    to the vocab size\n",
    "- If generating, $p = \\text{softmax}(x)$, and the next index is drawn from the \n",
    "    distribution $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, n_head: int, block_size: int, \n",
    "        vocab_size: int, n_layer: int, dropout: float, bias=True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.wte = nn.Embedding(vocab_size, n_embed)\n",
    "        self.wpe = nn.Embedding(block_size, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.h = [Block(n_embed, n_head, dropout, bias) for _ in range(n_layer)]\n",
    "        self.ln_f = LayerNorm(n_embed, bias=bias)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.wte.weight = self.lm_head.weight\n",
    "\n",
    "        def init_weights(_, m: nn.Module):\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Embedding):\n",
    "                m.weight = nn.init.normal(0.0, 0.02)(m.weight)\n",
    "                if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                    m.bias = mx.zeros_like(m.bias)\n",
    "        \n",
    "\n",
    "        self.apply_to_modules(init_weights)\n",
    "                \n",
    "    \n",
    "\n",
    "    def __call__(self, x_idx: mx.array):\n",
    "        _, T = x_idx.shape\n",
    "\n",
    "        assert T <= self.block_size, \\\n",
    "            f\"cannot forward sequence of length {T}, block size is only {self.block_size}\"\n",
    "        \n",
    "        pos = mx.arange(0, T, dtype=mx.int64)\n",
    "\n",
    "        tok_emb = self.wte(x_idx) # shape (B, T, C)\n",
    "        pos_emb = self.wpe(pos) # shape (T, C)\n",
    "\n",
    "        # (B, T, C) + (T, C) = (B, T, C)\n",
    "        # elementwise addition for each batch\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        for blk in self.h:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def generate(self, x_idx: mx.array, max_new_tokens: int, temperature=1.0):\n",
    "        # Take a conditioning sequence of indices x_idx (int64 tensor of shape (B, T)) and \n",
    "        # complete the sequence max_new_tokens times, feeding the predictions back into \n",
    "        # the model each time. Most likely you\"ll want to make sure to be in model.eval() \n",
    "        # mode of operation for this.\n",
    "        for _ in range(max_new_tokens):\n",
    "            if x_idx.shape[1] <= self.block_size:\n",
    "                x_idx_cropped = x_idx \n",
    "            else:\n",
    "                x_idx_cropped = x_idx[:, -self.block_size:]\n",
    "\n",
    "            logits = self(x_idx_cropped)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            next_idx = mx.random.categorical(logits)[None]\n",
    "            x_idx = mx.concatenate((x_idx, next_idx), axis=1)\n",
    "        return x_idx  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_INTERVAL = 500\n",
    "LOG_INTERVAL = 50\n",
    "\n",
    "BLOCK_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "MAX_ITERS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to blocks\n",
    "\n",
    "$x_i = [d_i, d_{i + 1}, ..., d_{i + b}]$\n",
    "\n",
    "$y_i = [d_{i + 1}, d_{i + 2}, ..., d_{i + b + 1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = block_data(train_data, BLOCK_SIZE)\n",
    "x_val, y_val = block_data(val_data, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model, optimizer, and training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeTransformer(\n",
    "    n_embed=768, \n",
    "    n_head=12, \n",
    "    block_size=BLOCK_SIZE, \n",
    "    vocab_size=50304,\n",
    "    n_layer=12, \n",
    "    dropout=0.0, \n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "# model.load_weights(\"checkpoints/model.npz\")\n",
    "\n",
    "optimizer = optimizers.AdamW(1e-3, (0.9, 0.95), 1e-7, 0.1)\n",
    "\n",
    "state = [model.state, optimizer.state]\n",
    "\n",
    "mx.eval(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy loss:\n",
    "\n",
    "$l(x, y, \\theta) = -\\sum_i y_i \\log(f(x_i, \\theta))$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Train Step with Adam Optimizer\n",
    "\n",
    "$g_t = \\nabla_{\\theta_{t - 1}} l(x, y, )$\n",
    "\n",
    "$\\alpha = \\eta \\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}$\n",
    "\n",
    "$m_t = \\beta_1 m_{t - 1} + (1 - \\beta_1)g_t$\n",
    "\n",
    "$m_t = \\beta_2 v_{t - 1} + (1 - \\beta_2)g_t^2$\n",
    "\n",
    "$\\theta_t = \\theta_{t - 1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    return nn.losses.cross_entropy(model(x), y, reduction=\"mean\")\n",
    "\n",
    "\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def train_step(x, y):\n",
    "    loss, grads = nn.value_and_grad(model, loss_fn)(model, x, y)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@partial(mx.compile, inputs=state)\n",
    "def eval_step(x, y):\n",
    "    return loss_fn(model, x, y)\n",
    "\n",
    "\n",
    "def evaluate_loss(x, y, max_iters=4):\n",
    "    loss_sum = 0\n",
    "    cnt = 0\n",
    "    for i, (bx, by) in enumerate(batch_iterate(x, y, BATCH_SIZE)):\n",
    "        if i >= max_iters:\n",
    "            break\n",
    "        loss = eval_step(bx, by)\n",
    "        loss_sum += loss * len(x)\n",
    "        cnt += len(x)\n",
    "    loss = loss_sum / cnt\n",
    "    mx.eval(loss)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  50], loss: 3.244, time: 56.876s\n",
      "[ 100], loss: 2.648, time: 56.662s\n",
      "[ 150], loss: 2.535, time: 56.323s\n",
      "[ 200], loss: 2.508, time: 55.374s\n",
      "[ 250], loss: 2.465, time: 56.055s\n",
      "[ 300], loss: 2.402, time: 56.962s\n",
      "[ 350], loss: 2.451, time: 55.487s\n",
      "[ 400], loss: 2.390, time: 55.237s\n",
      "[ 450], loss: 2.390, time: 56.033s\n",
      "[ 500], loss: 2.377, time: 54.835s\n",
      "train loss: 2.3687, val loss: 2.3775\n",
      "[ 550], loss: 2.363, time: 59.173s\n",
      "[ 600], loss: 2.346, time: 55.418s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_step(x, y)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "t0 = time.time()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "while True:\n",
    "    for x, y in batch_iterate(x_train, y_train, batch_size=BATCH_SIZE):\n",
    "        if i >= MAX_ITERS:\n",
    "            break\n",
    "        \n",
    "        if optimizer.learning_rate > 1e-6:\n",
    "            optimizer.learning_rate *= 0.999\n",
    "\n",
    "        loss = train_step(x, y)\n",
    "        mx.eval(state)\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"[{i:4}] loss: {loss.item():.3f}, time: {dt:.3f}s\")\n",
    "        \n",
    "        if i % EVAL_INTERVAL == 0:\n",
    "            train_loss = evaluate_loss(x_train, y_train)\n",
    "            val_loss = evaluate_loss(x_val, y_val)\n",
    "            print(f\"train loss: {train_loss:.4f}, val loss: {val_loss:.4f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                model.save_weights(f\"checkpoints/model.npz\")   \n",
    "\n",
    "        i += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your nan han hay hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hy hay han h han han h han h han h han h han h han h han h han h h han h han h han h han h han h han h h h h han h han h h h h h h h h wh h h h h h h h h h h h h h h h h h h h h h h wh wh h wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh whin wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh wh w\n"
     ]
    }
   ],
   "source": [
    "context = mx.array(encode(\"What is your na\"))[None]\n",
    "output = model.generate(context, max_new_tokens=512, temperature=1e-4)\n",
    "mx.eval(output)\n",
    "print(decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
