{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, activations, losses, optimizers\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are chars, so the vocab size is the number of unique chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char2int = {c: i for i, c in enumerate(chars)}\n",
    "int2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    return [char2int[c] for c in s if c in char2int]\n",
    "\n",
    "\n",
    "def decode(y: list[int] | np.ndarray | tf.Tensor) -> str:\n",
    "    return \"\".join([int2char[int(i)] for i in y if int(i) in int2char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text is encoded as an `Tensor`, then split into training and validation\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.array(encode(text), dtype=np.int64)\n",
    "\n",
    "val_size = len(full_data) // 10\n",
    "\n",
    "train_data = full_data[val_size:]\n",
    "val_data = full_data[:val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into blocks\n",
    "\n",
    "$x_i = [d_i, d_{i + 1}, ..., d_{i + b}]$\n",
    "\n",
    "$y_i = [d_{i + 1}, d_{i + 2}, ..., d_{i + b + 1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_data(data, block_size):\n",
    "    n_blocks = len(data) - block_size - 1\n",
    "    x = np.stack([data[i:i + block_size] for i in range(n_blocks)])\n",
    "    y = np.stack([data[i:i + block_size] for i in range(1, n_blocks + 1)])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random batches for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterate(x, y, batch_size):\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    for s in range(0, y.shape[0], batch_size):\n",
    "        idxs = permutation[s:s + batch_size]\n",
    "        yield tf.convert_to_tensor(x[idxs]), tf.convert_to_tensor(y[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(keras.Layer):\n",
    "    def __init__(self, dropout, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.c_fc = layers.Dense(\n",
    "            4 * input_shape[-1], \n",
    "            activation=activations.gelu,\n",
    "            use_bias=self.use_bias\n",
    "        )\n",
    "        self.c_proj = layers.Dense(\n",
    "            input_shape[-1], \n",
    "            activation=activations.gelu,\n",
    "            use_bias=self.use_bias\n",
    "        )\n",
    "        self.dropout = layers.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(keras.Layer):\n",
    "    def __init__(self, num_heads, dropout, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-5, center=self.use_bias)\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=input_shape[-1] // self.num_heads,\n",
    "            value_dim=input_shape[-1],\n",
    "            dropout=self.dropout,\n",
    "            use_bias=self.use_bias,\n",
    "            output_shape=input_shape[-1:]\n",
    "        )\n",
    "        self.ln_2 = layers.LayerNormalization(center=self.use_bias)\n",
    "        self.mlp = MLP(dropout=self.dropout, use_bias=self.use_bias)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.ln_1(x)\n",
    "        x = x + self.attn(x, x, use_causal_mask=True)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeTransformer(keras.Layer):\n",
    "    def __init__(\n",
    "        self, vocab_size, block_size, embedding_size,\n",
    "        num_heads, num_layers, dropout, use_bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "\n",
    "    def build(self, _):\n",
    "        self.wte = layers.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.wpe = layers.Embedding(self.block_size, self.embedding_size)\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "        block_args = (self.num_heads, self.dropout_rate, self.use_bias)\n",
    "        self.h = [Block(*block_args) for _ in range(self.num_layers)]\n",
    "        self.ln_f = layers.LayerNormalization(epsilon=1e-5, center=self.use_bias)\n",
    "        self.lm_head = layers.Dense(\n",
    "            self.vocab_size, \n",
    "            activation=activations.gelu,\n",
    "            use_bias=False,\n",
    "        )\n",
    "\n",
    "        # self.lm_head.weights[0] = self.wte.weights[0]\n",
    "\n",
    "    \n",
    "    def call(self, x_idx):\n",
    "            _, T = x_idx.shape\n",
    "\n",
    "            # assert T <= self.block_size, \\\n",
    "            #     f\"cannot forward sequence of length {T}, block size is only {self.block_size}\"\n",
    "            \n",
    "            pos = tf.range(0, T, dtype=tf.int64)\n",
    "\n",
    "            tok_emb = self.wte(x_idx) # shape (B, T, C)\n",
    "            pos_emb = self.wpe(pos) # shape (T, C)\n",
    "\n",
    "            # (B, T, C) + (T, C) = (B, T, C)\n",
    "            # elementwise addition for each batch\n",
    "            x = self.dropout(tok_emb + pos_emb)\n",
    "            for blk in self.h:\n",
    "                x = blk(x)\n",
    "            x = self.ln_f(x)\n",
    "            x = self.lm_head(x)\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_INTERVAL = 2500\n",
    "LOG_INTERVAL = 500\n",
    "\n",
    "BLOCK_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "MAX_ITERS = 10000\n",
    "\n",
    "MAX_LR = 1e-4\n",
    "WARMUP_ITERS = 100\n",
    "LR_DECAY_ITERS = 2500\n",
    "MIN_LR = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to blocks\n",
    "\n",
    "$x_i = [d_i, d_{i + 1}, ..., d_{i + b}]$\n",
    "\n",
    "$y_i = [d_{i + 1}, d_{i + 2}, ..., d_{i + b + 1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = block_data(train_data, BLOCK_SIZE)\n",
    "x_val, y_val = block_data(val_data, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32,), dtype=tf.int64)\n",
    "outputs = GenerativeTransformer(\n",
    "    vocab_size=vocab_size, \n",
    "    block_size=BLOCK_SIZE, \n",
    "    embedding_size=640,\n",
    "    num_heads=4, \n",
    "    num_layers=4, \n",
    "    dropout=0.0, \n",
    "    use_bias=True,\n",
    ")(inputs)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=losses.sparse_categorical_crossentropy)\n",
    "\n",
    "optimizer = optimizers.AdamW(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate loss from tensors $x, y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(x, y, max_iters=100):\n",
    "    loss_sum = 0\n",
    "    cnt = 0\n",
    "    for i, (bx, by) in enumerate(batch_iterate(x, y, BATCH_SIZE)):\n",
    "        if i >= max_iters:\n",
    "            break\n",
    "    \n",
    "        logits = model(bx)\n",
    "        logits = tf.reshape(logits, (-1, logits.shape[-1]))\n",
    "        by = tf.reshape(by, (-1,))\n",
    "        loss = model.loss(by, logits)\n",
    "        loss_sum += loss.numpy()[0] * len(x)\n",
    "        cnt += len(x)\n",
    "    return loss_sum / cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change learning rate over time\n",
    "\n",
    "$\\eta_i = \\begin{cases}\n",
    "    \\frac{\\eta \\cdot i}{N_{\\text{warmup}}} & i < N_{\\text{warmup}} \\\\\n",
    "    \\eta_{\\text{min}} + \\left(\n",
    "        \\frac{1}{2} + \\frac{1}{2}\\cos\\left(\n",
    "            \\pi \\frac{N_{\\text{warmup}} \\cdot i}{N_{\\text{decay}} - N_{\\text{warmup}}}\n",
    "        \\right)\n",
    "    \\right)(\\eta_0 - \\eta_{\\text{min}}) & N_{\\text{warmup}} \\leq i < N_{\\text{decay}} \\\\\n",
    "    \\eta_{\\text{min}} & N_{\\text{decay}} \\leq i\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(iter_num: int) -> float:\n",
    "    if iter_num < WARMUP_ITERS: \n",
    "        return MAX_LR * iter_num / WARMUP_ITERS \n",
    "    \n",
    "    if iter_num > LR_DECAY_ITERS:\n",
    "        return MIN_LR\n",
    "    \n",
    "    decay_ratio = (iter_num - WARMUP_ITERS) / (LR_DECAY_ITERS - WARMUP_ITERS)\n",
    "    assert 0 <= decay_ratio and decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + np.cos(np.pi * decay_ratio))\n",
    "    return MIN_LR + coeff * (MAX_LR - MIN_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([get_lr(i) for i in range(1, MAX_ITERS + 1)])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy loss:\n",
    "\n",
    "$l(x, y, \\theta) = -\\sum_i y_i \\log(f(x_i, \\theta))$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Train Step with Adam Optimizer\n",
    "\n",
    "$g_t = \\nabla_{\\theta_{t - 1}} l(x, y, )$\n",
    "\n",
    "$\\alpha = \\eta \\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}$\n",
    "\n",
    "$m_t = \\beta_1 m_{t - 1} + (1 - \\beta_1)g_t$\n",
    "\n",
    "$m_t = \\beta_2 v_{t - 1} + (1 - \\beta_2)g_t^2$\n",
    "\n",
    "$\\theta_t = \\theta_{t - 1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        logits = tf.reshape(logits, (-1, logits.shape[-1]))\n",
    "        y = tf.reshape(y, (-1,))\n",
    "        loss = model.loss(y, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply(grads, model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "t0 = time.time()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "while True:\n",
    "    if i > MAX_ITERS:\n",
    "        break\n",
    "    \n",
    "    for x, y in batch_iterate(x_train, y_train, batch_size=BATCH_SIZE):\n",
    "        if i > MAX_ITERS:\n",
    "            break\n",
    "\n",
    "        optimizer.learning_rate = get_lr(i)\n",
    "        loss = train_step(x, y)\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            print(f\"[{i:4}] loss: {loss.numpy()[0]:.3f}, time: {dt:.3f}s\")\n",
    "        \n",
    "        if i % EVAL_INTERVAL == 0:\n",
    "            train_loss = evaluate_loss(x_train, y_train)\n",
    "            val_loss = evaluate_loss(x_val, y_val)\n",
    "            print(f\"    train loss: {train_loss:.3f}, val loss: {val_loss:.3f}\")\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
